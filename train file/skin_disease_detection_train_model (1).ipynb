{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyxVsETw5EpK",
        "outputId": "4b3ad8e5-095b-4d74-e8ee-3c1cd1bc8475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "ra7Fok7W5I9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhRf7zm-5RoV",
        "outputId": "dea6d980-333c-4e05-8add-8b72972a0e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((64, 64)),  # Resize images\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "DHL6JyKw5UTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/Skin disease'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "                  for x in ['train', 'test']}\n",
        "\n",
        "# Create dataloaders\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
        "               for x in ['train', 'test']}\n",
        "\n",
        "# Get dataset sizes\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "\n",
        "# Get class names\n",
        "class_names = image_datasets['train'].classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Print for verification\n",
        "print(f\"Class Names: {class_names}\")\n",
        "print(f\"Train Size: {dataset_sizes['train']}\")\n",
        "print(f\"Test Size: {dataset_sizes['test']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaLREOQE5Yl6",
        "outputId": "ce68aab3-aae6-471b-91fe-20511581b121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Names: ['Acne and Rosacea Photos', 'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions', 'Atopic Dermatitis Photos', 'Bullous Disease Photos', 'Cellulitis Impetigo and other Bacterial Infections', 'Eczema Photos', 'Exanthems and Drug Eruptions', 'Hair Loss Photos Alopecia and other Hair Diseases', 'Herpes HPV and other STDs Photos', 'Light Diseases and Disorders of Pigmentation', 'Lupus and other Connective Tissue diseases', 'Melanoma Skin Cancer Nevi and Moles', 'Nail Fungus and other Nail Disease', 'Poison Ivy Photos and other Contact Dermatitis', 'Psoriasis pictures Lichen Planus and related diseases', 'Scabies Lyme Disease and other Infestations and Bites', 'Seborrheic Keratoses and other Benign Tumors', 'Systemic Disease', 'Tinea Ringworm Candidiasis and other Fungal Infections', 'Urticaria Hives', 'Vascular Tumors', 'Vasculitis Photos', 'Warts Molluscum and other Viral Infections']\n",
            "Train Size: 15741\n",
            "Test Size: 4012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg_F9hZJ5BZ9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EnhancedCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EnhancedCNN, self).__init__()\n",
        "\n",
        "        # Block 1\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # Output: 32x32x32\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 32x16x16\n",
        "\n",
        "        # Block 2\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # Output: 64x16x16\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 64x8x8\n",
        "\n",
        "        # Block 3\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Output: 128x8x8\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 128x4x4\n",
        "\n",
        "        # Block 4 (Additional Layers)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # Output: 256x4x4\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 256x2x2\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.pool1(F.leaky_relu(self.bn1(self.conv1(x))))\n",
        "        # Block 2\n",
        "        x = self.pool2(F.leaky_relu(self.bn2(self.conv2(x))))\n",
        "        # Block 3\n",
        "        x = self.pool3(F.leaky_relu(self.bn3(self.conv3(x))))\n",
        "        # Block 4\n",
        "        x = self.pool4(F.leaky_relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        # Flatten and Fully Connected Layers\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of classes (update this based on your dataset)\n",
        "num_classes = 23\n",
        "\n",
        "# Initialize the model\n",
        "model = EnhancedCNN(num_classes=num_classes).to(device)\n",
        "\n",
        "# Print the model architecture for verification\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSrP9tZ-6GmI",
        "outputId": "5cdb47e0-3f1e-4b3b-8baa-0630273c9fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EnhancedCNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=512, out_features=23, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam, SGD with momentum, or others)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler (optional for adaptive learning rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Decay LR every 7 epochs\n"
      ],
      "metadata": {
        "id": "89fCzYOU6Mto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations for data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Dataloader dictionary\n",
        "dataloaders = {'train': train_loader, 'test': test_loader}\n",
        "\n",
        "# Dataset sizes\n",
        "dataset_sizes = {'train': len(train_dataset), 'test': len(test_dataset)}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LddYhP0S6Oxo",
        "outputId": "f48f61c9-91e1-4f70-f35b-c923430e9521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 92.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "    best_model_wts = model.state_dict()\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "       # Each epoch has a training and testing phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Scheduler step\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = model.state_dict()\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
        "    print(f'Best test Acc: {best_acc:.4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model, criterion, optimizer, scheduler, num_epochs=15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZaoxUJL66f5",
        "outputId": "da93ece3-8aff-431f-cfea-e0196a3d295a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 1.3565 Acc: 0.5043\n",
            "test Loss: 1.2264 Acc: 0.5739\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 1.0467 Acc: 0.6298\n",
            "test Loss: 0.9939 Acc: 0.6527\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.9065 Acc: 0.6864\n",
            "test Loss: 0.8778 Acc: 0.6926\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.8183 Acc: 0.7172\n",
            "test Loss: 0.7866 Acc: 0.7243\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.7574 Acc: 0.7417\n",
            "test Loss: 0.6913 Acc: 0.7646\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.7032 Acc: 0.7583\n",
            "test Loss: 0.7024 Acc: 0.7652\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.6614 Acc: 0.7726\n",
            "test Loss: 0.6415 Acc: 0.7757\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.5434 Acc: 0.8123\n",
            "test Loss: 0.5504 Acc: 0.8131\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.5131 Acc: 0.8238\n",
            "test Loss: 0.5335 Acc: 0.8124\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.4979 Acc: 0.8278\n",
            "test Loss: 0.5261 Acc: 0.8220\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.4838 Acc: 0.8339\n",
            "test Loss: 0.5209 Acc: 0.8232\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.4767 Acc: 0.8338\n",
            "test Loss: 0.5083 Acc: 0.8252\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.4669 Acc: 0.8394\n",
            "test Loss: 0.5120 Acc: 0.8248\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.4554 Acc: 0.8428\n",
            "test Loss: 0.5137 Acc: 0.8282\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.4442 Acc: 0.8466\n",
            "test Loss: 0.5018 Acc: 0.8307\n",
            "\n",
            "Training complete in 55.0m 44.96707773208618s\n",
            "Best test Acc: 0.8307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate accuracy on the test dataset\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RO-1i186gZ9",
        "outputId": "2b43cd49-5799-4208-9517-3a5a2d15c00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 83.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model to Google Drive\n",
        "save_path = '/content/drive/My Drive/skin disease detection final model using enhanced cnn.pth'\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved successfully at {save_path}!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTwmzmCF6hfH",
        "outputId": "137116d5-e81e-4cf3-f962-75f73b3c6fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully at /content/drive/My Drive/skin disease detection final model using enhanced cnn.pth!\n"
          ]
        }
      ]
    }
  ]
}